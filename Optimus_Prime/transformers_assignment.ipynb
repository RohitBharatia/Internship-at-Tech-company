{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.691947Z",
     "start_time": "2025-04-25T09:48:22.677705Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.703565Z",
     "start_time": "2025-04-25T09:48:22.699136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim, num_heads=1):\n",
    "        super(SimpleSelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.out_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(x.size(-1))\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        attention_out = torch.matmul(attention, v)\n",
    "        return self.out_proj(attention_out)\n"
   ],
   "id": "eb59c81478ab867d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.719079Z",
     "start_time": "2025-04-25T09:48:22.716532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_attention():\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    embedding_dim = 8\n",
    "\n",
    "    x = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "    attention = SimpleSelfAttention(embedding_dim)\n",
    "\n",
    "    output = attention(x)\n",
    "    expected_shape = (batch_size, seq_len, embedding_dim)\n",
    "    assert output.shape == expected_shape, f\"expected shape {expected_shape}, got {output.shape}\"\n",
    "    print(\"attention test passed\")"
   ],
   "id": "afd18a3d446bdb9b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.734049Z",
     "start_time": "2025-04-25T09:48:22.730498Z"
    }
   },
   "cell_type": "code",
   "source": "test_attention()",
   "id": "30289bcb5188b339",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention test passed\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.766680Z",
     "start_time": "2025-04-25T09:48:22.744318Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset('rotten_tomatoes')",
   "id": "74f170f06af532ae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rotten_tomatoes couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/rohit/.cache/huggingface/datasets/rotten_tomatoes/default/0.0.0/aa13bc287fa6fcab6daf52f0dfb9994269ffea28 (last modified on Fri Apr 25 14:50:07 2025).\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.778946Z",
     "start_time": "2025-04-25T09:48:22.776764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ],
   "id": "1629d87ee78d2e58",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.792412Z",
     "start_time": "2025-04-25T09:48:22.789813Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset[0])",
   "id": "9e5b6c22802da848",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.845843Z",
     "start_time": "2025-04-25T09:48:22.816982Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")",
   "id": "ff1ec30dae184afe",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.851650Z",
     "start_time": "2025-04-25T09:48:22.849630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n"
   ],
   "id": "9becc43a1fa4eb2c",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.873415Z",
     "start_time": "2025-04-25T09:48:22.863333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train = train_dataset.map(tokenize, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize, batched=True)"
   ],
   "id": "814f663ec8ac4fd",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.894855Z",
     "start_time": "2025-04-25T09:48:22.890618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "id": "ad50d1e4bdaf8a79",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.905540Z",
     "start_time": "2025-04-25T09:48:22.899776Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_train[1]",
   "id": "c070821c2f6d43d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor(1),\n",
       " 'input_ids': tensor([  101,  1996,  9882,  2135,  9603, 13633,  1997,  1000,  1996,  2935,\n",
       "          1997,  1996,  7635,  1000, 11544,  2003,  2061,  4121,  2008,  1037,\n",
       "          5930,  1997,  2616,  3685, 23613,  6235,  2522,  1011,  3213,  1013,\n",
       "          2472,  2848,  4027,  1005,  1055,  4423,  4432,  1997,  1046,  1012,\n",
       "          1054,  1012,  1054,  1012, 23602,  1005,  1055,  2690,  1011,  3011,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:48:22.918326Z",
     "start_time": "2025-04-25T09:48:22.916549Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a1a5065d1d724a94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:56:34.203888Z",
     "start_time": "2025-04-25T09:56:34.200349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout = 0.3, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(self.dropout(cls_embedding))"
   ],
   "id": "4fc4d7b3f8b7448b",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:56:36.338219Z",
     "start_time": "2025-04-25T09:56:36.334368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=16)"
   ],
   "id": "c4b722cb50221c85",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T09:56:38.996599Z",
     "start_time": "2025-04-25T09:56:37.561969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ],
   "id": "a89ac843f7e8f526",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:25:53.776144Z",
     "start_time": "2025-04-25T09:56:40.407903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch {epoch + 1} — Loss: {avg_loss:.4f} — Accuracy: {accuracy:.2%}\")"
   ],
   "id": "ac668141cbb3f3ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 — Loss: 0.4228 — Accuracy: 79.95%\n",
      "Epoch 2 — Loss: 0.2273 — Accuracy: 91.03%\n",
      "Epoch 3 — Loss: 0.1109 — Accuracy: 95.80%\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:36:41.570398Z",
     "start_time": "2025-04-25T10:36:41.565437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = ['This movie was fantastic! I really enjoyed it.',\n",
    "               'Terrible movie. Waste of time and money.',\n",
    "               'It was okay. Not too good not too bad.']"
   ],
   "id": "8dc1507a43c334d2",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:36:43.150030Z",
     "start_time": "2025-04-25T10:36:43.145370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_batch(text_list):\n",
    "    # Tokenize the entire list at once\n",
    "    encoding = tokenizer(\n",
    "        text_list,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Run model in eval mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"]\n",
    "        )\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    # Map class index to label\n",
    "    label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    return [label_map[p.item()] for p in preds]"
   ],
   "id": "1f7b4369fd6bff10",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T10:36:45.669369Z",
     "start_time": "2025-04-25T10:36:45.541691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = predict_batch(sample_text)\n",
    "\n",
    "for text, label in zip(sample_text, predictions):\n",
    "    print(f\"\\\"{text}\\\" → {label}\")"
   ],
   "id": "90b9678d580ff126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This movie was fantastic! I really enjoyed it.\" → Positive\n",
      "\"Terrible movie. Waste of time and money.\" → Negative\n",
      "\"It was okay. Not too good not too bad.\" → Negative\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
